{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Part 1: Detecting Spam with Spark\n",
    "\n",
    "IN432 Big Data coursework 2017, part 1. Classifying messages to detect spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task a) & b) Read some files and prepare a (f,w) RDD \n",
    "a) Start by reading the directory with text files from the distributed file system (e.g. `hdfs://saltdean.nsqdc.city.ac.uk./data/spam/bare/part1`), and loading all text files using wholeTextFiles(), which loads the text per file, i.e. tuples (f,t). (5%)\n",
    "\n",
    "b) Split the text into words (lower case), creating a (file,word) RDD. (10%)\n",
    "\n",
    "For both tasks you can use the code from the labs. Don't remove finals 's' (we have already lemmatised data to work with later). \n",
    "\n",
    "It is very useful to pack the code into a function that takes a directory as an argument and returns an RDD with (f,w) structure, e.g. `read_fw_RDD`.\n",
    "\n",
    "Please write two lines of code at the end of the cell that run a little example and print some output. You can comment them out after you have verified that your code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "dirPath = 'hdfs://saltdean.nsqdc.city.ac.uk./data/spam/bare/part1'\n",
    "\n",
    "def read_fw_RDD( argDir ): # package tasks a/b into a function for later use\n",
    "    ... #<<< task a) read the files\n",
    "    print('Read {} files from directory {}'.format(<no of files>,argDir)) # status message for testing, can be disabled later on\n",
    "    #print('file word count histogram') # the histogram can be useful for checking later \n",
    "    #print(fwL_RDD.map(lambda fwL: (len(fwL[1]))).histogram([0,10,100,1000,10000]))\n",
    "    ... #<<< task b) split words\n",
    "    return fw_RDD #,fw_RDD\n",
    "\n",
    "fw_RDD = read_fw_RDD(dirPath[0:1]) # for testing\n",
    "print(fw_RDD.take(3)) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task c) Normalised word count lists\n",
    "Use the code from the labs to generate the \\texttt{[(word,count), ...]} list per file and to create a word frequency vector. \n",
    "\n",
    "Normalise the term frequency (TF) vector by the total word count per file. (15\\%)\n",
    "\n",
    "This is mostly reusing the lab code. The interesting part here is the normalisation. For normalisation we need to total word count per file. You can use a nested list comprehension for this (go through the (w,c) list and divide each c by the sum of all c, which you can get with a list). \n",
    "\n",
    "Another option is to use a separate RDD with (f,twc), where 'twc' is for total word count, and which you can create from the (f,[(w,c), ... ]) RDD. This new RDD can then be joined with the (f,[(w,c), ... ]) RDD and then the (w,c) list be normalised in a list comprehension. \n",
    "\n",
    "It would actually be more efficient to do the word counting when you split the words, but that would mix the tasks too much, so please don't do that in your coursework submission.\n",
    "\n",
    "Again, put your code into a function, and add a short test that can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "def reGrpLst(fw_c): # reorganise the tuples\n",
    "    ... #<<<  \n",
    "    \n",
    "def make_f_tfLn_RDD(argDir): \n",
    "    fw_RDD = read_fw_RDD( argDir ) # call function from task a & b\n",
    "    ... #<<< read as in the labs \n",
    "    ... #<<< and normalise by \n",
    "    return f_wcLn_RDD\n",
    "\n",
    "f_wcLn_RDD = make_f_tfLn_RDD('hdfs://saltdean/data/spam/bare/part5') # for testing\n",
    "#print(f_wcLn_RDD.take(1)) # for testing\n",
    "wcLn = f_wcLn_RDD.take(1)[0][1] # get the first normalised word count list\n",
    "print(sum([cn for (w,cn) in wcLn])) # the sum of normalised counts should be close to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task d) Creating hashed feature vectors \n",
    "Use the hashing trick to create fixed size TF vectors. (10%)\n",
    "\n",
    "Use the code from the week 2 lecture to create the hash vectors.\n",
    "\n",
    "As before, make it a function and add a short test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashing_vectorizer(word_count_list, N): \n",
    "    # use the code from the lecture\n",
    "\n",
    "def make_f_wVn_RDD(f_wcLn_RDD, argN):\n",
    "    # apply hashing_vectorizer\n",
    "    \n",
    "N=100\n",
    "#f_wVn_RDD = make_f_wVn_RDD(make_f_tfLn_RDD(dirPath),N) # for testing\n",
    "#print(f_wVn_RDD.take(1)[0][1]) # for testing\n",
    "#print( sum(f_wVn_RDD.take(1)[0][1])) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task e) Create Labeled Points\n",
    "\n",
    "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (ham) accordingly. Use map() to create an RDD of LabeledPoint objects. \n",
    "\n",
    "See here [http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) for an example, and here [http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) for the `LabeledPoint` documentation. (15%)\n",
    "\n",
    "It's useful to take the RDD with normalised word lists as input. \n",
    "\n",
    "For finding the spam messages use `re.search()` see here[https://docs.python.org/3/library/re.html?highlight=re%20search#re.search](https://docs.python.org/3/library/re.html?highlight=re%20search#re.search) for documentation. Search for 'spmsg' in the filename and check whether the result is `None`. The relevan syntax here is <b>`0 if <yourCondition> else 1`</b>, i.e. 0 if 'spmsg' is not in the filename (not spam) and 1 if it is (it's spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def make_lp_RDD(f_tfLn_RDD,argN):\n",
    "    #<<< make a vector\n",
    "    #<<< detect spam by filename \n",
    "    #<<< transform into LabeledPoint objects\n",
    "    return lp_RDD\n",
    "\n",
    "#lp_RDD = make_lp_RNN(make_f_tfLn_RDD('hdfs://saltdean.nsqdc.city.ac.uk./data/spam/bare/part1'),100)\n",
    "#print(lp_RDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task f) Train a classifier \n",
    "\n",
    "Use the `LabeledPoint` objects to train the `LogisticRegression` and calculate the accuracy of the model on the training set (again, follow this example [http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) and here is the documentation [http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS](http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS).  (15%) \n",
    "\n",
    "It's useful to start with a normalised word list as input again (because we can later also use it with TF.IDF values).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, NaiveBayes\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "path = 'hdfs://saltdean/data/spam/stop/part1'\n",
    "\n",
    "N=100\n",
    "def trainModel(f_wcL_RDD,N):\n",
    "    ... #<<< get the training data as LabeledPoint objects.\n",
    "    print('training data items: {}, correct: {}'.format(trainData.count(), correct)) # output raw numbers\n",
    "    print('training accuracy {:.1%}'.format(accuracy)) # and accuracy\n",
    "    return model \n",
    "\n",
    "f_wcLn_RDD = make_f_wcLn_RDD(path) # for testing\n",
    "model = trainModel(f_wcLn_RDD,N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task g) Test the classifier\n",
    "\n",
    "Use the files from \\texttt{.../data/extra/spam/bare/part10} and prepare them like in task~a)-e) (use the function you created in task e) and before. Then use the trained model to predict the label for each vector you have and compare it to the original to test the performance of your classifier. (10\\%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testModel(model,f_wcL_RDD,N):\n",
    "    #<<< like with trainModel, transform the data and evaluate it.\n",
    "    print('test data items: {}, correct:{}'.format(testData.count(),correct))\n",
    "    print('testing accuracy {:.1%}'.format(accuracy))\n",
    "\n",
    "testModel(model,make_f_wcLn_RDD('hdfs://saltdean/data/spam/stop/part10'),N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task h) Run experiments \n",
    "\n",
    "Package the whole classifier training and evaluation in one function. Then apply it to the files from `/data/extra/spam/lemm`, `/data/extra/spam/stop` and `/data/extra/spam/lemm_stop` in addition to `/data/extra/spam/bare`  and evaluate the accuracy of your classifier. \n",
    "\n",
    "Comment on the effect of *lemmatisation* and *stopword removal* on classification accuracy. Further, evaluate the use of larger training sets and the effect of different vector sizes. Print out the results of your experiments in readable form. (20%) \n",
    "\n",
    "You need to create one small fuction that combines tasks f) and g), and then apply it to different datasets sizes, vector sizes, and different preprocessings. \n",
    "\n",
    "The combination of the part1-part9 datasets can be achieved by using 'glob' patterns in the filename ('part[1-9]'). This is a feature of the Hadoop filesystem and not well documented in Spark (or anywhere else). You can find a description of its Python implementation here: [https://docs.python.org/3/library/glob.html](https://docs.python.org/3/library/glob.html). You can also supply multiple comma-separated paths, but you'll need to test what works, when you use this feature. Recursive patterns don't seem to work.\n",
    "\n",
    "Alternatively, you can create unions of RDDs for each part. However, this seems to lead to slower execution. With the latter, it is useful to created arrays of the directory names (part1, ...). When you work with unions, it may be useful to start with an empty RDD. That can be created with `sc.parallelize([])`.\n",
    "\n",
    "A useful tool for creatng multiple long paths variants is the use of the Python string format() method as used below. There is a good set of example here: [https://docs.python.org/3/library/string.html#format-examples](https://docs.python.org/3/library/string.html#format-examples) and the specification is here: [https://docs.python.org/3/library/string.html#format-specification-mini-language](https://docs.python.org/3/library/string.html#format-specification-mini-language).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-abdd88114391>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-abdd88114391>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    trainPath = dirPattern.format{i} # in the loop you can create a pattern like this\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# this function combines tasks f) and g)\n",
    "def trainTestModel(trainPaths,testPath,N):\n",
    "    ... #<<< just combine training ans testing here\n",
    "    \n",
    "# prepare the part directories and the path\n",
    "dirPattern = 'hdfs://saltdean/data/spam/bare/path[1-{}]' # the {} can be filled by 'dirpath.format(i)' \n",
    "# create the path for the test set\n",
    "testPath = 'hdfs://saltdean/data/spam/bare/path10'\n",
    "\n",
    "print('EXPERIMENT 1: Testing different training set sizes')\n",
    "print('Path = {}, N = {}'.format(dirpath,N)) # using format to make sure we record the parameters of the experiment\n",
    "#<<< make the test set, it will be constant for this experiment\n",
    "#<<< loop over i the number of parts for training (1-9)\n",
    "    trainPath = dirPattern.format{i} # in the loop you can create a path like this\n",
    "    print(trainPath) #just for testing, remove later\n",
    "    #<<< create the trainRDD (using your make_f_tfLn_RDD method)\n",
    "    trainTestModel(trainPaths,testPath,N)\n",
    "\n",
    "print('\\nEXPERIMENT 2: Testing different vector sizes')\n",
    "#<<< loop over different values for N. 3,10,30,100,300, ... is a good pattern\n",
    "    print('=== N = ',N)\n",
    "    trainTestModel(trainPaths,testPath,N)\n",
    "\n",
    "N = 100 # change to what you feel is a good compromise between computation and accuracy\n",
    "# the dictionary below helps associate description and paths.\n",
    "setDict = {'No preprocessing':'hdfs://saltdean/data/spam/bare/',\n",
    "           'Stopwords removed':'hdfs://saltdean/data/spam/stop/',\n",
    "           'Lemmatised':'hdfs://saltdean/data/spam/lemm/',\n",
    "           'Lemmatised and stopwords removed':'hdfs://saltdean/data/spam/lemm_stop/'}\n",
    "print('\\nEXPERIMENT 3: Testing differently preprocessed data sets')\n",
    "print('training on parts 1-9, N = {}'.format(N))\n",
    "for sp in setDict:\n",
    "    print('=== ',sp)\n",
    "    trainPath = setDict[sp]+'part[1-9]'\n",
    "    #<<< make the test and training data RDD and evaluate \n",
    "\n",
    "print('\\n====== Done ======')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here's an exampe what the output could look like\n",
    "\n",
    "Experiment 1: Testing differddent training set sizes\n",
    "Path = hdfs://saltdean/data/spam/lemm_stop/, N = 1000\n",
    "=== add part  1\n",
    "training data items: 289, correct: 289\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:270\n",
    "testing accuracy 92.8%\n",
    "=== add part  2\n",
    ".\n",
    ".\n",
    ".\n",
    "=== add part  9\n",
    "training data items: 2602, correct: 2602\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:285\n",
    "testing accuracy 97.9%\n",
    "\n",
    "Experiment 2: Testing different vector sizes\n",
    "Path = hdfs://saltdean/data/spam/lemm_stop/, training on parts1-9\n",
    "=== N =  3\n",
    "training data items: 2602, correct: 2171\n",
    "training accuracy 83.4%\n",
    "test data items: 291, correct:237\n",
    "testing accuracy 81.4%\n",
    "=== N =  10\n",
    ".\n",
    ".\n",
    ".\n",
    "=== N =  10000\n",
    "training data items: 2602, correct: 2602\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:284\n",
    "testing accuracy 97.6%\n",
    "\n",
    "Experiment 3: Testing differently processed data sets\n",
    "training on parts 1-9, N = 100\n",
    "===  No proprocessing\n",
    ".\n",
    ".\n",
    ".\n",
    "====== Done ======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task i) (Task for pairs) TF.IDF vectors\n",
    "You need to address this task if you are working as a pair. \n",
    "\n",
    "Calculate the IDF values for each word and generate fixed size TF.IDF vectors for each document (word frequencies still normalised by total document word count). Also evaluate the use of TF.IDF compared to normalised word counts in terms of accuracy. (25%)\n",
    "\n",
    "To calculate the IDF values you need to create an RDD (w,f) pairs. You can use the function `RDD.distinct()` to remove duplicates and reorganise to create (w,[f, ...]) lists. The length of the list is the document frequency and can be used to calculate the IDF.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from math import log\n",
    "\n",
    "trainPath = 'hdfs://saltdean.nsqdc.city.ac.uk./data/spam/lemm_stop/part[1-9]'\n",
    "testPath = 'hdfs://saltdean.nsqdc.city.ac.uk./data/spam/lemm_stop/part10'\n",
    "\n",
    "def make_f_wtfiL_RDD(path):\n",
    "    # Calculuate the IDFs\n",
    "    fw_RDD = read_fw_RDD(path)\n",
    "    #<<< keep only unique (f,w) pairs\n",
    "    #<<< (f,w) -> (w,[f])\n",
    "    #<<< join the lists of files with reduceByKey\n",
    "    vocSize = wfL_RDD.count() # calculate the vocabulary size\n",
    "    print('vocSize: {}'.format(vocSize)) \n",
    "    # calculate the IDF values per word by using len() on the list of files\n",
    "    print('wIdf_RDD.count(): ',wIdf_RDD.count()) # for testing\n",
    "    print(wIdf_RDD.take(2)) # for testing\n",
    "\n",
    "    # Get the normalise word counts (TFs) and organise by word (w,(f,cn))\n",
    "    f_wcLn_RDD = make_f_wcLn_RDD(path) # create the normalised word count lists \n",
    "    #print('f_wcLn_RDD: ',f_wcLn_RDD.map(\n",
    "    #        lambda x: sum([c for (w,c) in x[1]]).histogram([0,10,100,1000,10000]))) # check for the per-file word counts\n",
    "    #<<<< create a list of tuples [(w,(f,cn)), ..] and use flatmap \n",
    "    print('w_fcn_RDD.count(): {}'.format(w_fcn_RDD.count())) # for testing\n",
    "    print(w_fcn_RDD.take(2)) # for testing\n",
    "\n",
    "    # now we can join the IFDs and TFs by the words (w,(f,cn)) join (w,idf) to (w,((f,cn),idf))\n",
    "    #<<< use RDD.join()\n",
    "    print( 'w_fcnIdf_RDD.count(): ', w_fcnIdf_RDD.count())\n",
    "    print( w_fcnIdf_RDD.take(2))\n",
    "\n",
    "    # we have doubly nested tuples (w,((f,cn),idf)) in the RDD, \n",
    "    # but they let us calculate the TF.IDF per file and word (f,[(w,cn*idf)]).\n",
    "    #<<<< map to (f,[(w,cn*idf)])\n",
    "    print('f_wtfiL_RDD.count()', f_wtfiL_RDD.count())\n",
    "    print(str(f_wtfiL_RDD.take(2)))\n",
    "\n",
    "    # with that we can reduce by key (files) to get [(w,tfidf), ...] lists per file.\n",
    "    #<<< reduceByKey\n",
    "    print('# of files with TF.IDF vectors: {}'.format(f_wtfiL2_RDD.count()))\n",
    "    print(f_wtfiL2_RDD.take(2)))\n",
    "\n",
    "    return f_wtfiL2_RDD\n",
    "\n",
    "\n",
    "N=100 # choose a value yourself \n",
    "print('N: {}, trainPath: {}'.format(N,trainPath))\n",
    "#<<< you can now apply trainModel and test Model to RDDs created with make_f_wtfiL_RDD()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
